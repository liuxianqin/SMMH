# 限制爬虫



## 来源审查

判断 User-Agent 只响应浏览器或者友好的爬虫



## 发布Robots协议

Robots Exclusion Standard

哪些可以，哪些不行

网站根目录下robots.txt



```
# 京东

User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

```Python
print("HELLO")
```

如果没有robots协议，那么就允许所有爬虫不限制访问。





### 对于一个网站，先自动识别或人工识别robots协议。



类似人的行为可以不用参考robots协议。访问内容少，数据量也不大。



